{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Puff Model\n",
    "\n",
    "Yunha Lee \n",
    "August 7, 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Step: Get weather data for the target area and times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Define the path and variables\n",
    "weather_file_names = ['u_wind', 'v_wind', 'wind_direction', 'vertical_velocity', 'temperature']\n",
    "data_dir = \"/Users/yunhalee/Documents/methanDart/UCAR_reanalysis/\"\n",
    "geopotential_file = f\"{data_dir}geopotential_height_NorthAmerica.nc\"\n",
    "\n",
    "# Define source locations and time range\n",
    "source_locs = pd.DataFrame({\n",
    "    'lat': [34.05, 34.06],\n",
    "    'lon': [-118.25, -118.26],\n",
    "    'height': [10, 50]  # Heights in meters\n",
    "})\n",
    "start_time = pd.to_datetime(\"2015-06-01 12:00:00\")\n",
    "end_time = pd.to_datetime(\"2015-06-10 12:00:00\")\n",
    "\n",
    "# Read geopotential height to find the corresponding pressure levels\n",
    "with xr.open_dataset(geopotential_file) as ds:\n",
    "    ds_gph = ds.sel(time=slice(start_time, end_time))\n",
    "\n",
    "    # Dictionary to hold DataFrames for each location\n",
    "    location_weather_data = {}\n",
    "\n",
    "    # Iterate over each location\n",
    "    for index, row in source_locs.iterrows():\n",
    "        loc_id = f\"lat{row['lat']}_lon{row['lon']}_height{row['height']}\"\n",
    "        loc_weather_data = pd.DataFrame()\n",
    "\n",
    "        # Iterate over each weather variable\n",
    "        for var_name in weather_file_names:\n",
    "            with xr.open_dataset(f\"{data_dir}{var_name}_NorthAmerica.nc\") as ds_var:\n",
    "                ds_var = ds_var.sel(time=slice(start_time, end_time))\n",
    "\n",
    "                # Get the nearest geopotential heights for this location and convert heights to pressure levels\n",
    "                gph_values = ds_gph.sel(latitude=row['lat'], longitude=row['lon'], method='nearest')\n",
    "\n",
    "                # Print dimensions to debug\n",
    "                print(\"Dimensions of gph_values:\", gph_values.dims)\n",
    "\n",
    "                # If multiple times are present, consider reducing them\n",
    "                if 'time' in gph_values.dims:\n",
    "                    gph_values = gph_values.isel(time=0)  # selecting the first time point for simplicity\n",
    "\n",
    "                # Continue with height comparison\n",
    "                height_diff = abs(gph_values['geopotential_height'] - row['height'])\n",
    "                nearest_pressure_level_idx = height_diff.argmin(dim='isobaricInhPa')\n",
    "                nearest_pressure_level = gph_values['isobaricInhPa'].isel(isobaricInhPa=nearest_pressure_level_idx).values\n",
    "\n",
    "                # Print the type and value to debug\n",
    "                print(\"Type of nearest_pressure_level:\", type(nearest_pressure_level))\n",
    "                print(\"Value of nearest_pressure_level:\", nearest_pressure_level)\n",
    "\n",
    "\n",
    "                var_data = ds_var.sel(latitude=row['lat'], longitude=row['lon'], isobaricInhPa=nearest_pressure_level, method='nearest')\n",
    "                df_var = var_data.to_dataframe()\n",
    "\n",
    "                # Renaming columns to reflect data specifics\n",
    "                new_column_names = {\n",
    "                    name: f\"{name}\"\n",
    "                    for name in df_var.columns\n",
    "                }\n",
    "\n",
    "                df_var.rename(columns=new_column_names, inplace=True)\n",
    "                if loc_weather_data.empty:\n",
    "                    loc_weather_data = df_var\n",
    "                else:\n",
    "                    loc_weather_data = pd.merge(loc_weather_data, df_var, how='outer', left_index=True, right_index=True, suffixes=('', '_duplicate'))\n",
    "\n",
    "                    # Optionally, drop duplicate columns if necessary\n",
    "                    duplicate_columns = [col for col in loc_weather_data.columns if '_duplicate' in col]\n",
    "                    loc_weather_data.drop(columns=duplicate_columns, inplace=True)\n",
    "\n",
    "        # Save the DataFrame for this location\n",
    "        location_weather_data[loc_id] = loc_weather_data\n",
    "\n",
    "        # Save to CSV\n",
    "        loc_weather_data.to_csv(f\"../input_data/{loc_id}_weather_data.csv\")\n",
    "\n",
    "# Print one of the DataFrames as an example\n",
    "for loc_id, df in location_weather_data.items():\n",
    "    print(f\"Data for {loc_id}:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the GPM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "from gaussian_puff_functions import *\n",
    "\n",
    "def simulate_puff_concentration(source_index, source_locs, data, dt, emission_rate, times, chunk_size, area_size, height_levels, n_chunks):\n",
    "    print(f\"Source: {source_index + 1}/{len(source_locs)}\")\n",
    "\n",
    "    # Source location and conversion to UTM\n",
    "    source_lat = source_locs.iloc[source_index]['lat']\n",
    "    source_lon = source_locs.iloc[source_index]['lon']\n",
    "    source_x, source_y = latlon_to_utm(source_lat, source_lon)\n",
    "    source_z = source_locs.iloc[source_index]['height']\n",
    "\n",
    "    # Define a 3D grid or area over which to calculate concentration\n",
    "    grid_x, grid_y, grid_z = np.meshgrid(np.linspace(source_x-area_size/2, source_x+area_size/2, num=30),\n",
    "                                         np.linspace(source_y-area_size/2, source_y+area_size/2, num=30),\n",
    "                                         np.linspace(0, source_z + height_levels, num=10))  \n",
    "\n",
    "    # Wind data and interpolation\n",
    "    WS_x = data['u_wind']\n",
    "    WS_y = data['v_wind']\n",
    "    WA = data['wind_direction']\n",
    "    WA_x = np.cos(WA)\n",
    "    WA_y = np.sin(WA)\n",
    "    WS = np.sqrt(WS_x**2 + WS_y**2)\n",
    "\n",
    "    # Emission rates\n",
    "    Q_truth = np.full((len(WS)), emission_rate)\n",
    "\n",
    "    # Setup chunk processing\n",
    "    n_ints = len(WS)-1 # ignore the last time \n",
    "\n",
    "    args = [(h, chunk_size, dt, n_ints, source_x, source_y, source_z, WS_x, WS_y, WS, Q_truth, grid_x, grid_y, grid_z, times) \n",
    "            for h in range(n_chunks)]\n",
    "    \n",
    "    print(\"args\", len(args))\n",
    "\n",
    "    print(f\"chunk_size: {chunk_size}, n_chunks: {n_chunks}, dt: {dt}, n_ints: {n_ints}, \"\n",
    "      f\"source_x: {source_x}, source_y: {source_y}, source_z: {source_z}, \"\n",
    "      f\"WS_x length: {len(WS_x)}, WS length: {len(WS)}, Q_truth: {Q_truth[0]}, \")\n",
    "\n",
    "    with Pool(processes=n_chunks) as pool:\n",
    "        results = pool.map(process_chunk, args)\n",
    "        #print(\"result\", results)\n",
    "\n",
    "    big_C = np.vstack(results)\n",
    "    return big_C\n",
    "\n",
    "\n",
    "\n",
    "##TODO GET_STABILITY_CLASS MUST TAKE SURFACE_WIND, not the U_Wind!! \n",
    "\n",
    "# Define source locations and time range\n",
    "source_locs = pd.DataFrame({\n",
    "    'lat': [34.05],\n",
    "    'lon': [-118.25],\n",
    "    'height': [10]  # Heights in meters\n",
    "})\n",
    "\n",
    "# Define the size of the area over which you want to simulate concentrations\n",
    "area_size = 100  # domain size in meters (grid size is 100 meter)\n",
    "height_levels = 100 # domain heigh in meters\n",
    "dt = 1  # sec\n",
    "emission_rate = 0.1  # kg/s\n",
    "\n",
    "# Determine the simulation duration using chunk_size and n_chunk (for multiprocessing)\n",
    "chunk_size = 10  # Number of minutes in each chunk\n",
    "n_chunks = 6 # Number of chunks (also, number of processors)\n",
    "\n",
    "start_time = pd.to_datetime(\"2015-06-01 12:00:00\")\n",
    "end_time = calculate_end_time(start_time, chunk_size, n_chunks)\n",
    "print(f\"The end time of the simulation will be: {end_time}\")\n",
    "\n",
    "# Create a sequence of minutes between start_time and end_time\n",
    "times = pd.date_range(start=start_time, end=end_time, freq=f'{dt}s')\n",
    "\n",
    "# Remove the last timestamp to exclude the last second (to make it clean number)\n",
    "times = times[:-1]\n",
    "\n",
    "print(times)\n",
    "\n",
    "output_dir = \"/Users/yunhalee/Documents/methanDart/Gaussian_Puff_CH4/output_data/\"\n",
    "\n",
    "# Prepare to store the results\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each location\n",
    "for s, row in source_locs.iterrows():\n",
    "    loc_id = f\"lat{row['lat']}_lon{row['lon']}_height{row['height']}\"\n",
    "    print(f\"Data for {loc_id}:\")\n",
    "\n",
    "    data = pd.read_csv (f\"../input_data/{loc_id}_weather_data.csv\")\n",
    "\n",
    "    # Convert the 'time' column to datetime if not already\n",
    "    data['time'] = pd.to_datetime(data['time'])\n",
    "    data.set_index('time', inplace=True)\n",
    "\n",
    "    # Resample the data to hourly from 3-hourly data\n",
    "    hourly_data = data.resample('1H').mean()\n",
    "    hourly_data = hourly_data.interpolate(method='linear')\n",
    "\n",
    "    # Filter the data between start_time and end_time\n",
    "    filtered_data = hourly_data[start_time:end_time]\n",
    "\n",
    "    resampled_data = filtered_data.resample(f'{dt}s').mean()\n",
    "    resampled_data = resampled_data.interpolate(method='linear')\n",
    "\n",
    "    # Print and/or process the interpolated data\n",
    "    print(\"Interpolated weather data:\", resampled_data.head())\n",
    "    print(\"Interpolated weather data:\", len(resampled_data))\n",
    "\n",
    "    big_C = simulate_puff_concentration(s, source_locs,  resampled_data, dt, emission_rate, times, chunk_size, area_size, height_levels, n_chunks)\n",
    "\n",
    "    plot_2d_concentration(big_C, times) \n",
    "\n",
    "    # Average the output\n",
    "    output_dt_minutes = 60  # Define how many seconds you want to average over\n",
    "    df_resampled_big_C = average_time_resolution(big_C, times, output_dt_minutes, output_dir, s, source_locs, area_size,height_levels)\n",
    "\n",
    "    plot_2d_concentration_from_df(df_resampled_big_C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
